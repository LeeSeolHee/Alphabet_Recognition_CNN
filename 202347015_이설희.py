# -*- coding: utf-8 -*-
"""202347015_이설희.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Yl2vVuM-mF7sTDfCL_treap-eN-cIUdB

## Import Libraries
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import cv2
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from keras.models import Sequential
from keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout
#from keras.optimizers import SGD, Adam
from tensorflow.keras.optimizers import SGD
from keras import optimizers
from keras.callbacks import ReduceLROnPlateau, EarlyStopping
from keras.utils import to_categorical

"""## Read dataset with pandas (python)"""

my_data = pd.read_csv('/content/drive/MyDrive/alphabet/A_Z Handwritten Data.csv').astype('float32')
my_frame = pd.DataFrame(my_data)
my_frame.head()

my_frame.shape

my_frame.info()

my_frame.isnull().sum()

"""## Split x and y column from dataset"""

x = my_frame.drop('0', axis = 1)
y = my_frame['0']

"""## Reshaping images from dataset"""

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2) # 20 / 80
x_train = np.reshape(x_train.values, (x_train.shape[0], 28, 28))
x_test = np.reshape(x_test.values, (x_test.shape[0], 28, 28))

print('Train Data Shape:', x_train.shape)
print('Test Data Shape:', x_test.shape)

"""## Creating dictionary  of alphabets with indexes"""

word_dict = {
    0:'A',1:'B',2:'C',3:'D',4:'E',5:'F',6:'G',7:'H',8:'I',9:'J',10:'K',11:'L',12:'M',13:'N',14:'O',15:'P',16:'Q',17:'R',18:'S',19:'T',20:'U',21:'V',22:'W',23:'X', 24:'Y',25:'Z'
}

"""## Plotting number of alphabets from dataset"""

# Plotting the number of alphabets in the dataset
# 1): Firstly we convert the labels into integer values and append into the count list according to the label.
#     This count list has the number of images present in the dataset belonging to each alphabet.
# 2): Now we create a list – alphabets containing all the characters using the values() function of the dictionary.
# 3): Now using the count & alphabets lists we draw the horizontal bar plot.

plt.style.use('fivethirtyeight')

y_integer = np.int0(y)
count = np.zeros(26, dtype = 'int')

for i in y_integer:
    count[i] += 1

alphabets = []

for i in word_dict.values():
    alphabets.append(i)

fig, ax = plt.subplots(1, 1, figsize = (15, 15))
ax.barh(alphabets, count)

plt.xlabel('Number Of Elements..!!', fontsize = 20, fontweight = 'bold', color = 'green')
plt.ylabel('Alphabets..!!', fontsize = 30, fontweight = 'bold', color = 'green')
plt.grid()
plt.show()

"""## Shuffle images from dataset with shuffle() function to show random images"""

# Now we shuffle the data with the shuffle() function to show the random images..

plt.style.use('fivethirtyeight')

shuff = shuffle(x_train[:100])
fig, ax = plt.subplots(3, 3, figsize = (15, 15))
axes = ax.flatten()

for i in range(9):
    shu = cv2.threshold(shuff[i], 30, 200, cv2.THRESH_BINARY)
    axes[i].imshow(np.reshape(shuff[i], (28, 28)), cmap = 'Greys')
plt.show()

"""## Reshaping train & test images from dataset to put in Deep Learning Model"""

x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)
print("New shape of train data:", x_train.shape)

x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)
print("New shape of test data:", x_test.shape)

"""## Convert float values into categorical values"""

categorical_train = to_categorical(y_train, num_classes = 26, dtype = 'int')
print("New shape of train labels:", categorical_train.shape)

categorical_test = to_categorical(y_test, num_classes = 26, dtype = 'int')
print("New shape of test labels:", categorical_test.shape)

"""## Implementation of Deep Learning Model"""

# Sequential() Model: 순차적 모형은 일반적인 층 쌓기에 적합합니다
# 각 레이어에 하나의 입력 텐서와 하나의 출력 텐서가 있는 경우.

# MaxPool2D: 입력창 위에 최대값을 가져감으로써 공간적 차원(높이와 너비)을 따라 입력합니다
#입력의 각 채널에 대해 #(pool_size로 정의된 크기). 창은 각 차원을 따라 보폭으로 이동됩니다.

# 모델 1
# my_model = Sequential()

# my_model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation = 'relu', input_shape = (28, 28, 1)))
# my_model.add(MaxPool2D(pool_size = (2, 2), strides = 2))

# my_model.add(Conv2D(filters = 64, kernel_size = (3, 3), activation = 'relu', padding = 'same'))
# my_model.add(MaxPool2D(pool_size = (2, 2), strides = 2))

# my_model.add(Conv2D(filters = 128, kernel_size = (3, 3), activation = 'relu', padding = 'valid'))
# my_model.add(MaxPool2D(pool_size = (2, 2), strides = 2))

# my_model.add(Flatten())

# my_model.add(Dense(64, activation = "relu"))
# my_model.add(Dense(128, activation = "relu"))
# my_model.add(Dense(26, activation = "softmax"))

# 모델 2
my_model = Sequential()
my_model.add(Conv2D(32, (3,3), input_shape = (28, 28, 1), activation='relu', padding='same'))
my_model.add(Conv2D(64, (3,3), activation='relu', padding='same'))
my_model.add(MaxPool2D(pool_size=(2,2)))
my_model.add(Dropout(0.25))

my_model.add(Flatten())
my_model.add(Dense(256, activation='relu'))
my_model.add(Dropout(0.25))
my_model.add(Dense(26, activation='softmax'))

"""## Compiling the Model"""

my_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])
history = my_model.fit(x_train, categorical_train, epochs = 1, validation_data = (x_test, categorical_test))

"""## Getting summary of Model"""

# model.save () 함수를 사용하여 모델을 저장합니다.

my_model.save('model_hand3.h5')

"""## Finally, make predictions on the test data"""

plt.style.use('fivethirtyeight')

fig, axes = plt.subplots(3, 3, figsize = (12, 15))
axes = axes.flatten()

for i, ax in enumerate(axes):
    img = np.reshape(x_test[i], (28, 28))
    ax.imshow(img, cmap = 'Greys')

    pred = word_dict[np.argmax(categorical_test[i])]
    ax.set_title("Prediction: " + pred, fontsize = 20, fontweight = 'bold', color = 'red')
    ax.grid()